
\chapter{The distributive property}

The distributive law will be the star of the tensor show.   Distributive laws
appear throughout our world of measurement and this offers some gentle points 
of entry to the study.  
Start with area.
\begin{center}
    \begin{tikzpicture}
        \draw[thick, fill=blue!20] (0,0) rectangle (1,1);
        \draw[thick, fill=ForestGreen!20] (1,0) rectangle (3,1);

        \node at ( 0.5,-0.5) {$w$};
        \node at ( 2,-0.5) {$\hat{w}$};
        \node at (-0.5,0.5) {$h$};

    \end{tikzpicture}
\hspace{1in}
\begin{tikzpicture}
    \draw[thick, fill=blue!20] (0,0) rectangle (1,1);
    \draw[thick, fill=ForestGreen!20] (0,1) rectangle (1,3);

    \node at ( 0.5,-0.5) {$w$};
    \node at (-0.5,0.5) {$h$};
    \node at (-0.5,2) {$\hat{h}$};

\end{tikzpicture}
\end{center}
So area as  function $A(w,h)$ of width and height distributes over those parameters.
\begin{align*}
    A(w+\hat{w},h) &= A(w,h)+A(\hat{w},h)
    &    A(w,h+\hat{h}) &= A(w,h)+A(w,\hat{h})
\end{align*}
Calculus says that all areas are defined by distributed sums 
of subrectangles, or by the steady approximation by such rectangles.
Thus area's distributive property water-marks calculus 
in numerous ways including the following fundamental identity.
\begin{align*}
    \int_M (f+g)\text{d}x = \int_M f \text{d}x + \int_M g \text{d}x
\end{align*}

\subsection{``Real'' life}
Look elsewhere in your life.  When you buy $x$ items at cost $\text{Cost}(x)$
and then buy $\acute{x}$ more the total cost is:
\[
    \text{Cost}(x+\acute{x})=\text{Cost}(x)+\text{Cost}(\acute{x}).
\]  
Add the cost of tax $\text{Tax}(x)$ and the total changes to 
\[
    (\text{Cost}+\text{Tax})(x)=\text{Cost}(x)+\text{Tax}(x).
\]
It is not just cost of one item.  Buy items $x_1,\ldots, x_n$ at 
costs $C_i(x_i)$ each and set 
\(
    \text{Cost}(x_*)=\text{Cost}(x_1,\ldots,x_n) = C_1(x_1)+\cdots +C_n(x_n)
\)
Still we find
$\text{Cost}(x_*+\acute{x}_*)=\text{C}(x_*)+\text{Cost}(\acute{x}_*)$.
Cost distributes.  

Here is another life situation.  The work of $e$ employee for $h$ hours produces 
$\text{W}(e,h)$ widgets
so at peak productivity adding $\hat{e}$ workers or $\hat{h}$ extra hours we get
\begin{align*}
    \text{W}(e+\hat{e},h)& =\text{W}(e,h)+\text{W}(\hat{e},h)
    &
    \text{W}(e,h+\hat{h})& =\text{W}(e,h)+\text{W}(e,\hat{h}).
\end{align*}
Work distributes, both over workers and over time.

% Imagine next the ingredient list 
% $\text{Supplies}(r)$ of recipe $r$.  If we need to shop for ingredients of two recipes 
% $r$ and $\acute{r}$ then 
% \[
%     \text{Supplies}(r+\acute{r})=\text{Supplies}(r)+\text{Supplies}(\acute{r}).
% \]
Need to print the names of both faculty and graduate students. 
You could distribute that work as well, as seen in Listing~\ref{lst:distributive},
the command \code{Print} distributes over the operator of concatenation \code{cat}
with the result of simply appending the two individual printing commands.
We could say 
\begin{center}
    \code{Print(as cat bs) == Print(as) append Print(bs)}
\end{center}

\begin{lstfloat}
\begin{notebookin}
fac = [ "Hamilton", "Levi-Chivita" ]
students = [ TBD use math geoneolgoy to find some students of these]
Print(faculty cat students)
\end{notebookin}
\begin{notebookout}
Hamilton
Levi-Chivita
....TBD
\end{notebookout}
\begin{notebookin}
Print(faculty)
Print(students)
\end{notebookin}
\begin{notebookout}
Hamilton
Levi-Chivita
....TBD
\end{notebookout}
\caption{Many operations in computation are distributive.}\label{lst:distributive}
\end{lstfloat}

Well all this is at least true for some range of values.  All scientific
claims of connection to reality are true only for sensible regions of values.

The distributive appears so often because we are willing to be flexible about 
what we want from addition and multiplication.  We just use those two names 
to remember ``multiplication distributes over addition''---whatever those two 
terms mean.

\subsection{Notations}

By custom, area $A(w,h)$ of rectangles is denoted $w \cdot h$ and called
multiplication.  In that notation we have the familiar form.
\begin{align*}
    (w+\hat{w})\cdot h & = w \cdot h +\hat{w}\cdot h 
    &
    w\cdot (h+\hat{h}) & = w\cdot h + w\cdot \hat{h}.
\end{align*}
This is the first of dozens of notations invented over the centuries to communicate 
functions that distribute their work amongst sums of sub-applications of the function.  
Most honor one of these three modes:
\begin{itemize}
    \item functions on the outside applied to sums of inputs 
    \[ 
        f(u+\acute{u})=f(u)+f(\acute{u}),\qquad 
        \code{Print(as cat bs)},\qquad
        \text{Cost}
        (x,y,z)\ldots;
    \]

    \item functions in the middle like these operators 
    \[
        (u+\acute{u})*v=u*v+\acute{u}*v, \qquad 
        (u+\acute{u})\cdot v=u\cdot v+\acute{u}\cdot v,\qquad 
        u\otimes v, \qquad 
        uv,\ldots; 
    \]

    \item functions on the outside like these
    \[
        \langle u+\acute{u}|v\rangle=\langle u|v\rangle +\langle \acute{u}|v\rangle,\qquad 
        [u+\acute{u},v]=[u,v]+[\acute{u},v],\qquad  
        /u,v/,\ldots~.
    \]
    
\end{itemize}
   
\subsection{Axes and dependent-functions}
Just, as area distributes, so does volume $V(\ell,w,h)$ only with one more axis.
\begin{center}
    \textbf{[TBD: a tikz picture of three volumes showing three axes of distributive volume]}
\end{center}
As formulas this could be written:
\begin{align*}
    V(\ell+\hat{\ell},w,h) & = V(\ell,w,h)+V(\hat{\ell},w,h)\\
        V(\ell,w+\hat{w},h) & = V(\ell,w,h)+V(\ell,\hat{w},h)\\
        V(\ell,w,h+\hat{h}) & = V(\ell,w,h)+V(\ell,w,\hat{h})
\end{align*}
With volumes we obviously must give up on the chiral language of ``distributes on the left''
``distributes on the right''.   Venturing into hypervolumes will be worse and benefits from some notation.  

First we need to name every coordinate and while we could number coordinates 
a for more flexible idea is the name them after what they mean.  For example 
the rows and columns of matrix are typically used in that order and rows 
are interpreted as moving top-to-bottom..  Meanwhile $x$ and $y$ flip this 
convention with the left-to-right going first and bottom-to-top orientation for 
vertical.  So a function $M_{rc}$ in row-column form is differently interpreted 
from a function $f_{xy}$ in $x$-axis, $y$-axis form.  So naming the axes 
rather than prescribing fixed meaning is the most common.

\marginpar{\begin{flushleft}Dependent-functions are elements of $\prod_{a\in A}U_a$.\end{flushleft}}
Now that the axes $A$ are named, when we want to changing value $x_a$ on an axis $a\in A$ 
we do so merely by providing an annotation, usually a subscript.
For example $A=\{\text{row},\text{col}\}$ means we could consider 
$u_A=(u_{\text{row}}, u_{\text{col}})$.  What is going on here is that we are not actually
describing an ordered pair but instead a function whose output type depends on the input
or what is more commonly known as a \emph{dependent-function}.  Order-pairs 
can be seen as special cases of dependent -functions, they depend on $A=\{\text{left},\text{right}\}$
or sometimes $\{1,2\}$, but the the convention for writing ordered pairs $(x,y)$ is that the 
position of the value is fixed and from that one recovers the implicit input, e.g. $x$ is in the left 
so implicitly $\text{left}\mapsto x$.

There are two notations used somewhat interhcangeably for dependent-functions
\begin{align}
    u_A & :a\mapsto u_a & 
    u_A & = (u_a\mid a\in A)
\end{align}
When it become tedious we write $u=u_A$.
If we partition $A=B\sqcup C$ we can accordingly restrict the dependent-function $u$ 
to the smaller domains creating two dependent-functions $u_B$ and $u_c$ with the same overall information,
which we write
\begin{align*}
    u_A & = (u_B, u_C)
\end{align*}
An especially important case is for $a\in A$ where we let $\bar{a}=A-\{a\}$ and write 
\begin{align*}
    u_A & = (u_a,u_{\bar{a}})
\end{align*}

Back to volume we could now have stated $A=\{\text{l},\text{w},\text{h}\}$ as the axes.
Then volume is a function $\text{Vol}(u_A)$ where for each $a\in A$, $u_a\geq 0$.  Thus 
the distributive property in all three variables can be condensed to a single formula 
quantified over every axis.
\begin{align*}
    & (\forall a\in A)
    &
    \text{Vol}(u_{a}+\acute{u}_a,u_{\bar{a}}) & = \text{Vol}(u_a,u_{\bar{a}}) + \text{Vol}(\acute{u}_a,u_{\bar{a}}) 
\end{align*}
Notice if we change $A=\{\text{w},\text{h}\}$ we get the distributive law for area, 
and if we change to $A=\{x,y,z\}$ we get volume but no in reference to xyz-coordinates
instead of the more ambiguous length-width-height coordinates.  Hypervolumes are 
then the same rule but with more axes such as $A=\{x,y,z,t\}$ or axes $A=\{x_1,\ldots, x_{\ell}\}$.

\emph{In these notes $u_A=(u_a\mid a\in A)$ denotes a dependent-function not an ordered tuple.}

\section{Distributive algebra}


What does distribution require?
\begin{align}
    u*(v+\acute{v}) & = u*v+u*\acute{v}
    & 
    (u+\acute{u})*v & = u*v + \acute{u}*v.
\end{align}
We definitely need additions, but we should not jump to assume that $u$, $v$, and $w$ are 
of the same type of data.  Just look at  matrix 
multiplication (we use $\mathbb{R}^{a\times b}$ to denote $(a\times b)$-matrices 
of real numbers)
\begin{align*}
    *&:\mathbb{R}^{a\times b}\times \mathbb{R}^{b\times c}\to \mathbb{R}^{a\times c}
    &
    (u*v)_{ij} & = \sum_k u_{ik}v_{kj}.
\end{align*}
So we except this is a study of \emph{heterogeneous} algebra, so we wont be
captivated by homomorphism but rather what we will call \emph{hetero}morphisms.
So we could think of three types of data $U$, $V$ and $W$ each with a $+$ each
combined by a function 
$*:U\times V\to W$ that satisfies the distributive law.  It turns out we often need a 
little more.

\begin{proposition}
    If $+$ is a binary operation on $U$ then we may add a term $0$ disjoint form $U$, 
    $U_0\defeq U\sqcup \{0\}$ with addition 
    \begin{align*}
        \begin{array}{|c|cc|}
            \hline 
            \dot{+} & 0 & b \\
            \hline 
            0 & 0 & b\\
            a & a & a+b \\
            \hline 
        \end{array}
    \end{align*}
    So every additive structure may be assumed to have a $0$ where $x+0=x=0+x$.
\end{proposition}


\begin{definition}
    Let $U$, $V$ and $W$ have additions $+$ (a binary operator) and a $0$.
    A \emph{bimap} (biadditive map) is a function $*:U\times V\bmto W$ where 
    \begin{align*}
        (u+\acute{u})* v & = u*v + \acute{u}*v \\
        u*(v+\acute{v}) & = u*v + u*\acute{v}\\
        0 * v & = 0 = u*v.
    \end{align*}
\end{definition}

As we go along we will prefer to use $U$, $V$ and $W$ in just this way so that 
we can get up to speed on examples as quickly as possible.

\subsection{Properties of addition}
It is tempting now to start  assuming that $U$, $V$, and $W$ are 
something family---vector spaces, modules, or at least abelian groups.
However this would rob the distributive law of its power and leave us to think 
addition and its common attributes are the reason tensors work.  But the 
distributive law is already claiming a strong interaction of two operations 
so maybe it should be explored on its own a little while longer to appreciate 
what it already says about the individual operations.  More examples will demonstrate 
the value of a general point of view.

We will use a number of spaces
\begin{align*}
    \mathbb{R}^d & \defeq \{u:[d]\to \mathbb{R}\},\\
    R^{m\times n} &\defeq \{M:[m]\times [n]\to \mathbb{R}\}\\
    R^{\ell\times m\times n} &\defeq \{\Gamma:[\ell]\times [m]\times [n]\to \mathbb{R}\}\\
    & \vdots
\end{align*}
Define the following operations.
\begin{gather*}
    \mathbb{R}^m\oplus \mathbb{R}^n  \defeq \mathbb{R}^{m+n}\\
    \begin{bmatrix} \mathbb{R}^{a\times n} \\ \mathbb{R}^{b\times n} \end{bmatrix} \defeq \mathbb{R}^{(a+b)\times n}
    \qquad
    \begin{bmatrix}
    \mathbb{R}^{m\times c} &
    \mathbb{R}^{m\times d}
    \end{bmatrix}  \defeq \mathbb{R}^{m\times (c+d)}\\
%     \mathbb{R}^{a\times m\times n} \oplus \mathbb{R}^{b\times m\times n}  \defeq \mathbb{R}^{(a+b)\times m\times n}
%     \qquad
%     \mathbb{R}^{\ell \times c\times n} \oplus \mathbb{R}^{\ell \times d\times n}  \defeq \mathbb{R}^{\ell\times (c+d)\times n}
%     \qquad
% \\  
%      \mathbb{R}^{\ell \times m\times e} \oplus \mathbb{R}^{\ell \times m\times f}  \defeq \mathbb{R}^{\ell\times m\times (e+f)}\\
     \vdots
\end{gather*}



Now we add a multiplication.
\begin{align*}
    \mathbb{R}^m \otimes \mathbb{R}^n & \defeq \mathbb{R}^{m\times n}
\end{align*}

\begin{example}
    The distributive law with vector space operators.
    \begin{align*}
    (\mathbb{R}^a \oplus \mathbb{R}^b)\otimes \mathbb{R}^n 
        % & = \mathbb{R}^{(a+b)\times c}
        &  = \begin{bmatrix}\mathbb{R}^{a}\otimes \mathbb{R}^n  \\ \mathbb{R}^b\otimes \mathbb{R}^n\end{bmatrix}\\
    \mathbb{R}^m\oplus ( \mathbb{R}^c \oplus \mathbb{R}^d )
        % & = \mathbb{R}^{(a+b)\times c}
        &  = \begin{bmatrix} 
            \mathbb{R}^{m}\otimes \mathbb{R}^c &
            \mathbb{R}^m\otimes \mathbb{R}^d
        \end{bmatrix}
    \end{align*}
\end{example}

Lets combine the left and right 
distributive laws.

\begin{center}
    \begin{tikzpicture}[xscale=2]
        \node (A) at (0,0) {$(u+\acute{u})*(v+\acute{v})$};
        \node (B) at (-2,-1) {$(u+\acute{u})*v+(u+\acute{u})*\acute{v}$};
        \node (C) at (2,-1) {$u*(v+\acute{v})+\acute{u}*(v+\acute{v})$};
        \node (D) at (-2,-2) {$(u*v+\acute{u}*v)+(u*\acute{v}+\acute{u}*\acute{v})$};
        \node (E) at (2,-2) {$(u*v+u*\acute{v})+(\acute{u}*v+\acute{u}*\acute{v})$};
        \draw[double distance=2pt] (A) -- (B);
        \draw[double distance=2pt] (A) -- (C);
        \draw[double distance=2pt] (B) -- (D);
        \draw[double distance=2pt] (C) -- (E);
    \end{tikzpicture}
\end{center}
Thus the values $a,b,c,d,\ldots $ in the image of a distributive product $*:U\times V\bmto W$ must 
satisfy the following identity.
\begin{equation}
    \tag{Distributable}
    (a+b)+(c+d) = (a+c)+(b+d)
\end{equation}

\begin{proposition}
    If $+$ is both associative and commutative then it is distributable.
    Furthermore, such an addition can be extended to have at $0$.

    Conversely, if $+$ is distributable and has a $0$ then $+$ is associative 
    and commutative.
\end{proposition}
\begin{proof}
    TBD
\end{proof}



\section{Commonoids are required}

\marginpar{$a\equiv \acute{a}\pod{R}$ is a congruence relation of an algebra.}
\marginpar{$\text{Fr}_{\Omega}\langle S\rangle$ is the subalgebra of evaluate 
all $\Omega$-formulas $\Phi(X)$ at values in $S$.  We say $S$ \emph{generates} this subalgebra.}
Now we get to tame the algebra.  
\begin{definition}
    Given a distributive product $*:U\times V\bmto W$
    let 
    \begin{align*}
        (u\equiv \acute{u} \pod{V^{\top}}) & \Leftrightarrow (\forall v\in V)(u*v=\acute{u}*v)\\
        (v\equiv \acute{v} \pod{U^{\bot}}) & \Leftrightarrow (\forall u\in U)(u*v=u*\acute{v})\\
        U*V & := \text{Fr}_{+,0}\langle u*v \mid u\in U, v\in V\rangle
    \end{align*}
    When necessary we denoted equivalence classes $u/{V^{\bot}}$ and etc.
\end{definition}

\begin{proposition}
    Each of the following induced products are well-defined and distributive.
    \begin{itemize}
        \item $*:U/V^{\top} \times V\bmto W$
        \item $*:U \times V/U^{\bot}\bmto W$
        \item $*:U \times V\bmto W^+$
        \item $*:U/V^{\top} \times V/U^{\bot}\bmto U*V$
    \end{itemize}
\end{proposition}
\begin{proof}
    TBD by a Wilsonite
\end{proof}

\begin{definition}
    An additive algebra that is both associative and commutative is called a \emph{commonoid}.
\end{definition}

\begin{proposition}
    Given a distributive product $*:U\times V\to W$ then 
    $U/V^{\bot}$, $V/U^{\top}$ and $W^+=U*V$ are commonoids.
\end{proposition}
\begin{proof}
    TBD by a Wilsonite
\end{proof}

In commutative diagram form (i.e.\ flow charts where any two paths between the same points agree) we have the following.
\begin{center}
    \begin{tikzcd}
        U\arrow[d,two heads] &[\elastic] 
        \times &[\elastic]
        V\arrow[d,two heads] \arrow[r,"*",tail] &
        W & (\text{Additive algebras})
      \\
        U/V^{\top} &
        \times &
        V/U^{\bot} \arrow[r,"\bullet",tail] &
        U*V\arrow[u,hook,"h"]
        & (\text{Commonoids})
      \end{tikzcd}
    % \begin{tikzcd}
    %     $U$ &[\elastic] $\times$ &[\elastic] $V$ \arrow[tail,r] & $W$\\
    % \end{tikzcd}
\end{center}
We have surjections and inclusion so something is lost, but none of it is information!
Notice $u\equiv \acute{u}\pod{V^{\top}}$ precisely when for \emph{every} $v\in V$,
$u*v=\acute{u}*v$.  So factoring this in does not influence those values.  Likewise 
with moding out by $U^{\bot}$.  Finally, $U*V$ is generated by the image of the function $*$ so 
anything in $W-U*V$ is something that the function ignores.

This says that every every bimap induces one that is defined commonoids and no information is lost.

[TBD: describe Tucker decomposition]

\subsection{Grothendieck group}

\begin{theorem}
    If $M$ is a commonoid then 
    \begin{align*}
        (m,n) \equiv (\acute{m},\acute{n}) \pod{Grth} & \Leftrightarrow (\exists k\in M)(m+\acute{n}+k=\acute{m}+n+k)
    \end{align*}
    is a congruence on $M\oplus M$.  Furthermore,
    \begin{align*}
        (m,n)+(n,m) \equiv (0,0) \pod{Grth}
    \end{align*}
    So $\Gr(M)\defeq M\oplus M/Grth$ is an abelian group where 
    \[
        -(m,n)\defeq (n,m)
    \]
    This is called the Grothendieck group of $M$.
    The interpretation is that $(m,n)/R$ is ``m-n''.  When $m=0$ it is customary to write $-n$.

    The homomorphism $M\to \Gr(M)$ given by $m\mapsto (m,0)$ is injective 
    if, and only if, $m+n=m+\acute{n}$ implies $m=m$, i.e.\ $M$ has cancellation.
\end{theorem}

\begin{proposition}
    If $u\in U$ with $-u$ such that $u+(-u)=0$ then 
    \[
        u*v + (-u)*v =0
    \]
    Thus if $U$ has negatives then we can promote these negatives to $U*V$.
    In particular $-(u*v)\defeq (-u)*v$ defines a negative in $W$ of $u*v$.
    We can do likewise in the variable $V$.  
\end{proposition}
Now imagine the distributive law was present along side an existing subtraction.
Then the following would be a consequence.
\begin{align*}
    (a-b)*(c-d) & = (a*(c-d))-(b*(c-d))
        = (a*c-a*d)-(b*c-b*d)\\
        & =(a*c+b*d)-(a*d+b*c)
\end{align*}
This should explain the following extension.

\begin{corollary}
    If $*:U\times V\bmto W$ is a distributive product of commonoids then there is an 
    induced distributive product 
    \begin{gather*}
        * :\Gr(U)\times \Gr(V)\bmto \Gr(W)\\
        (u-\acute{u})*(v-\acute{v})
        \defeq (u*v+\acute{u}*\acute{v})-(u*\acute{v}+\acute{u}*v).
    \end{gather*}
    In particular $(-u)*v=-(u*v)=u*(-v)$.
\end{corollary}

Let us stand back to see what we have built.

\begin{center}
    \begin{tikzcd}
        U\arrow[d,two heads] &[\elastic] 
        \times &[\elastic]
        V\arrow[d,two heads] \arrow[r,"*",tail] &
        W & (\text{Additive algebras})
      \\
        U/V^{\top}\arrow[d] &
        \times &
        V/U^{\bot}\arrow[d] \arrow[r,"\bullet",tail] &
        U*V\arrow[u,hook,"h"]\arrow[d]
        & (\text{Commonoids})\\
        \Gr(U/V^{\top}) &
        \times &
        \Gr(V/U^{\bot}) \arrow[r,"\bullet",tail] &
        \Gr(U*V)
        & (\text{Abelian Groups})
      \end{tikzcd}
    % \begin{tikzcd}
    %     $U$ &[\elastic] $\times$ &[\elastic] $V$ \arrow[tail,r] & $W$\\
    % \end{tikzcd}
\end{center}
It is not a mistake that in the last component the arrows are not aligned.
This is actually a symptom of an unresolved problem in the study of tensors.
They do not lie in a single category.  


\section{The tensor lifting property}



\section{The tensor product}